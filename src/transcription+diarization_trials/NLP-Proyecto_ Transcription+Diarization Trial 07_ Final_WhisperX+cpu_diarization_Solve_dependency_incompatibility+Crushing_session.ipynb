{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyMxmL/r3+cV1seE0TZnXVU5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["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"],"metadata":{"id":"CWQ7mPzgqT9t"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dKYkQNlKonVy","outputId":"f8e920d2-34e4-4c87-d05a-f4a23b7705c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","Paste your Hugging Face token (it will be hidden):\n","HUGGINGFACE_TOKEN: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n","üîΩ Updating yt-dlp and attempting to download audio...\n","> yt-dlp -f bestaudio -x --audio-format wav -o 'content/audio_raw.wav' 'https://www.youtube.com/watch?v=LJiUDxj-2ZE'\n","‚úÖ Direct audio extraction succeeded: content/audio_raw.wav\n","> ffmpeg -y -i 'content/audio_raw.wav' -ar 16000 -ac 1 -vn -acodec pcm_s16le 'content/audio.wav'\n","‚úÖ Re-encoded extracted wav to 16k mono: content/audio.wav\n","‚úÇÔ∏è Splitting audio into 10-minute segments to avoid memory pressure...\n","> ffmpeg -hide_banner -loglevel error -i 'content/audio.wav' -f segment -segment_time 600 -c copy content/segments/segment_%03d.wav\n","üîé 5 segment(s) to process.\n","üéß Loading WhisperX model 'tiny' on cuda ...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/pyannote/audio/core/io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n","  torchaudio.list_audio_backends()\n","/usr/local/lib/python3.12/dist-packages/speechbrain/utils/torch_audio_backend.py:57: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n","  available_backends = torchaudio.list_audio_backends()\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n","/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["2025-10-15 16:30:01 - whisperx.asr - INFO - No language specified, language will be detected for each audio file (increases inference time)\n","2025-10-15 16:30:01 - whisperx.vads.pyannote - INFO - Performing voice activity detection using Pyannote...\n"]},{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../usr/local/lib/python3.12/dist-packages/whisperx/assets/pytorch_model.bin`\n"]},{"output_type":"stream","name":"stdout","text":["Model was trained with pyannote.audio 0.0.1, yours is 3.4.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n","Model was trained with torch 1.10.0+cu102, yours is 2.8.0+cu126. Bad things might happen unless you revert torch to 1.x.\n","\n","--- Transcribing segment 1/5: content/segments/segment_000.wav ---\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n","It can be re-enabled by calling\n","   >>> import torch\n","   >>> torch.backends.cuda.matmul.allow_tf32 = True\n","   >>> torch.backends.cudnn.allow_tf32 = True\n","See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n","\n","  warnings.warn(\n"]}],"source":["# ============================================================\n","# WORKING COLAB: YouTube -> robust downloader -> WhisperX + Pyannote diarization\n","# Single cell (paste & run in Colab; set Runtime -> GPU)\n","# ============================================================\n","\n","# ----------------- Installs -----------------\n","# update pip (optional), install latest yt-dlp and models\n","!pip install -q --upgrade pip\n","!pip install -q -U yt-dlp\n","!pip install -q git+https://github.com/m-bain/whisperX.git\n","!pip install -q \"pyannote.audio>=2.1\" ffmpeg-python faster_whisper\n","\n","# make sure ffmpeg binary available\n","!apt-get update -qq && apt-get install -y -qq ffmpeg\n","\n","# ----------------- Imports & config -----------------\n","import os, shlex, subprocess, json, csv, math\n","from getpass import getpass\n","import torch\n","\n","# -------------- USER CONFIG --------------\n","YOUTUBE_URL = \"https://www.youtube.com/watch?v=LJiUDxj-2ZE\"  # <-- change if desired\n","WHISPER_MODEL = \"tiny\"   # tiny recommended for Colab; change if you want\n","OUT_DIR = \"content\"\n","os.makedirs(OUT_DIR, exist_ok=True)\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","# -----------------------------------------\n","\n","# -------------- Hugging Face token --------------\n","if \"HUGGINGFACE_TOKEN\" not in os.environ:\n","    print(\"Paste your Hugging Face token (it will be hidden):\")\n","    os.environ[\"HUGGINGFACE_TOKEN\"] = getpass(\"HUGGINGFACE_TOKEN: \")\n","HUGGINGFACE_TOKEN = os.environ[\"HUGGINGFACE_TOKEN\"]\n","\n","# ----------------- robust downloader -----------------\n","video_path = os.path.join(OUT_DIR, \"video.mp4\")\n","wav_path_raw = os.path.join(OUT_DIR, \"audio_raw.wav\")   # direct extraction may create this\n","audio_16k = os.path.join(OUT_DIR, \"audio.wav\")          # final 16k mono file\n","\n","print(\"üîΩ Updating yt-dlp and attempting to download audio...\")\n","\n","def run(cmd):\n","    print(\">\", cmd)\n","    proc = subprocess.run(shlex.split(cmd), capture_output=True, text=True)\n","    if proc.returncode != 0:\n","        print(\"Command failed with return code\", proc.returncode)\n","        print(\"STDOUT:\", proc.stdout)\n","        print(\"STDERR:\", proc.stderr)\n","        raise subprocess.CalledProcessError(proc.returncode, cmd)\n","    return proc\n","\n","# Attempt direct audio extraction to WAV (preferred)\n","try:\n","    # try extracting & re-encoding to wav in one step\n","    cmd = f\"yt-dlp -f bestaudio -x --audio-format wav -o '{wav_path_raw}' '{YOUTUBE_URL}'\"\n","    run(cmd)\n","    print(\"‚úÖ Direct audio extraction succeeded:\", wav_path_raw)\n","except subprocess.CalledProcessError:\n","    print(\"‚ö†Ô∏è Direct extraction failed ‚Äî falling back to download then ffmpeg convert.\")\n","    try:\n","        # download bestaudio into a container (mp4 or webm)\n","        cmd = f\"yt-dlp -f bestaudio -o '{video_path}' '{YOUTUBE_URL}'\"\n","        run(cmd)\n","        print(\"‚úÖ Video/audio downloaded to:\", video_path)\n","        # convert to 16k mono WAV PCM\n","        ffmpeg_cmd = f\"ffmpeg -y -i '{video_path}' -ar 16000 -ac 1 -vn -acodec pcm_s16le '{audio_16k}'\"\n","        run(ffmpeg_cmd)\n","        print(\"‚úÖ Converted to 16k mono WAV:\", audio_16k)\n","    except subprocess.CalledProcessError as e:\n","        raise RuntimeError(\"Failed to download or convert the YouTube video. See error above.\") from e\n","else:\n","    # If direct extraction produced wav_path_raw, convert it to 16k mono pcm_s16le for consistency\n","    try:\n","        ffmpeg_cmd = f\"ffmpeg -y -i '{wav_path_raw}' -ar 16000 -ac 1 -vn -acodec pcm_s16le '{audio_16k}'\"\n","        run(ffmpeg_cmd)\n","        print(\"‚úÖ Re-encoded extracted wav to 16k mono:\", audio_16k)\n","    except subprocess.CalledProcessError:\n","        raise RuntimeError(\"Failed to re-encode extracted audio to 16k mono.\")\n","\n","# ----------------- split long audio into segments (10 min) -----------------\n","print(\"‚úÇÔ∏è Splitting audio into 10-minute segments to avoid memory pressure...\")\n","seg_dir = os.path.join(OUT_DIR, \"segments\")\n","os.makedirs(seg_dir, exist_ok=True)\n","# segment_time seconds (600 = 10 minutes). Reduce if necessary.\n","SEGMENT_TIME = 600\n","split_cmd = f\"ffmpeg -hide_banner -loglevel error -i '{audio_16k}' -f segment -segment_time {SEGMENT_TIME} -c copy {seg_dir}/segment_%03d.wav\"\n","# If ffmpeg can't copy segments for pcm, we fallback to re-encoding for each segment\n","try:\n","    run(split_cmd)\n","except subprocess.CalledProcessError:\n","    # fallback: create segments by re-encoding\n","    print(\"‚ö†Ô∏è fallback segmentation (re-encoding per segment)\")\n","    run(f\"ffmpeg -y -i '{audio_16k}' -f segment -segment_time {SEGMENT_TIME} -ar 16000 -ac 1 {seg_dir}/segment_%03d.wav\")\n","segments = sorted([os.path.join(seg_dir, f) for f in os.listdir(seg_dir) if f.startswith(\"segment_\")])\n","if len(segments) == 0:\n","    # if segmentation produced nothing, just use the full file\n","    segments = [audio_16k]\n","print(f\"üîé {len(segments)} segment(s) to process.\")\n","\n","# ----------------- WhisperX transcription (per-segment) -----------------\n","import whisperx\n","import torch\n","print(f\"üéß Loading WhisperX model '{WHISPER_MODEL}' on {DEVICE} ...\")\n","model = whisperx.load_model(WHISPER_MODEL, device=DEVICE)   # tiny recommended\n","all_segments = []\n","detected_language = None\n","\n","# load alignment model lazily per detected language later; we'll collect everything then align per segment\n","for idx, segfile in enumerate(segments):\n","    print(f\"\\n--- Transcribing segment {idx+1}/{len(segments)}: {segfile} ---\")\n","    try:\n","        res = model.transcribe(segfile, batch_size=16)\n","    except Exception as e:\n","        # sometimes giving file path instead of loaded audio works better\n","        print(\"Transcription error, retrying passing loaded audio directly...\", e)\n","        audio = whisperx.load_audio(segfile)\n","        res = model.transcribe(audio, batch_size=16)\n","    if detected_language is None:\n","        detected_language = res.get(\"language\")\n","    # align this segment's segments to word-level\n","    print(\"‚è±Ô∏è Running alignment for word-level timestamps...\")\n","    model_a, metadata = whisperx.load_align_model(language_code=res[\"language\"], device=DEVICE)\n","    aligned = whisperx.align(res[\"segments\"], model_a, metadata, segfile, device=DEVICE, return_char_alignments=False)\n","    # keep the segments but adjust start times with segment offset (if segment file not starting at 0)\n","    # For safety, assume segments start at 0; we will later compute absolute times by adding segment offsets only if ffmpeg created contiguous segments at correct offsets.\n","    all_segments.extend(aligned[\"segments\"])\n","\n","print(\"\\n‚úÖ Completed WhisperX transcription + alignment for all segments.\")\n","# Save intermediate whisperx JSON\n","whisper_json = os.path.join(OUT_DIR, \"whisperx_transcript.json\")\n","with open(whisper_json, \"w\", encoding=\"utf-8\") as f:\n","    json.dump({\"language\": detected_language, \"segments\": all_segments}, f, ensure_ascii=False, indent=2)\n","print(\"Saved WhisperX aligned segments:\", whisper_json)\n","\n","# ----------------- Pyannote diarization -----------------\n","print(\"\\nüë• Loading pyannote speaker-diarization pipeline...\")\n","from pyannote.audio import Pipeline\n","try:\n","    pipeline = Pipeline.from_pretrained(\n","    \"pyannote/speaker-diarization\",\n","    use_auth_token=HUGGINGFACE_TOKEN,\n","    device=\"cpu\"   # <-- force CPU for diarization\n",")\n","except Exception as e:\n","    raise RuntimeError(\"Failed to load pyannote pipeline. Ensure your Hugging Face token is correct and has access.\") from e\n","\n","print(\"üïí Running diarization on full audio (this can take a bit)...\")\n","try:\n","    diarization = pipeline(audio_16k)\n","except Exception as e:\n","    # If diarization fails due to memory, try diarizing per segment and stitch (best-effort)\n","    print(\"‚ö†Ô∏è Diarization on full audio failed, attempting per-segment diarization as fallback:\", e)\n","    from pyannote.core import Annotation, Segment as PSegment\n","    combined = Annotation()\n","    speaker_counter = 0\n","    for i, segfile in enumerate(segments):\n","        print(f\"  diarizing segment {i+1}/{len(segments)}: {segfile}\")\n","        try:\n","            ann = pipeline(segfile)\n","        except Exception as e2:\n","            print(\"    failed for this segment, skipping:\", e2)\n","            continue\n","        # rename local speakers to avoid collisions (best-effort)\n","        mapping = {}\n","        for turn, _, label in ann.itertracks(yield_label=True):\n","            new_label = f\"S{i:02d}_{label}\"\n","            combined[turn] = new_label\n","    diarization = combined\n","    print(\"‚ö†Ô∏è Per-segment diarization completed (labels may be per-segment).\")\n","\n","# Save RTTM\n","rttm_path = os.path.join(OUT_DIR, \"diarization.rttm\")\n","with open(rttm_path, \"w\", encoding=\"utf-8\") as f:\n","    diarization.write_rttm(f)\n","print(\"Saved diarization RTTM:\", rttm_path)\n","\n","# ----------------- Merge WhisperX words (word-level) with diarization -----------------\n","# Build word list from whisperx result\n","words = []\n","# whisperx stores words inside each segment's 'words' if alignment succeeded\n","for seg in all_segments:\n","    if \"words\" in seg:\n","        for w in seg[\"words\"]:\n","            # ensure numeric times\n","            start = float(w.get(\"start\", seg.get(\"start\", 0.0)))\n","            end = float(w.get(\"end\", seg.get(\"end\", 0.0)))\n","            words.append({\"word\": w.get(\"word\") or w.get(\"text\") or \"\", \"start\": start, \"end\": end})\n","    else:\n","        # fallback to segment-level text\n","        start = float(seg.get(\"start\", 0.0))\n","        end = float(seg.get(\"end\", 0.0))\n","        words.append({\"word\": seg.get(\"text\", \"\").strip(), \"start\": start, \"end\": end})\n","\n","# Convert diarization to list of segments with labels\n","dia_segs = []\n","for turn, _, label in diarization.itertracks(yield_label=True):\n","    dia_segs.append({\"start\": float(turn.start), \"end\": float(turn.end), \"speaker\": label})\n","\n","# helper to assign speaker by max overlap\n","def assign_speaker_for_word(w_start, w_end, dia_segs):\n","    best_speaker = \"SPEAKER_UNKNOWN\"\n","    best_overlap = 0.0\n","    for seg in dia_segs:\n","        overlap = max(0.0, min(w_end, seg[\"end\"]) - max(w_start, seg[\"start\"]))\n","        if overlap > best_overlap:\n","            best_overlap = overlap\n","            best_speaker = seg[\"speaker\"]\n","    return best_speaker\n","\n","for w in words:\n","    w[\"speaker\"] = assign_speaker_for_word(w[\"start\"], w[\"end\"], dia_segs)\n","\n","# Save merged outputs\n","merged_json = os.path.join(OUT_DIR, \"merged_words_with_speakers.json\")\n","with open(merged_json, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(words, f, ensure_ascii=False, indent=2)\n","print(\"Saved merged words with speakers:\", merged_json)\n","\n","csv_path = os.path.join(OUT_DIR, \"merged_words_with_speakers.csv\")\n","with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n","    writer = csv.DictWriter(f, fieldnames=[\"start\", \"end\", \"speaker\", \"word\"])\n","    writer.writeheader()\n","    for w in words:\n","        writer.writerow({\"start\": w[\"start\"], \"end\": w[\"end\"], \"speaker\": w[\"speaker\"], \"word\": w[\"word\"]})\n","print(\"Saved merged CSV:\", csv_path)\n","\n","# ----------------- Merge words into readable speaker segments and SRT -----------------\n","merged_segments = []\n","current = None\n","for w in words:\n","    if current is None:\n","        current = {\"speaker\": w[\"speaker\"], \"start\": w[\"start\"], \"end\": w[\"end\"], \"text\": w[\"word\"]}\n","    elif w[\"speaker\"] == current[\"speaker\"] and w[\"start\"] - current[\"end\"] <= 1.0:\n","        current[\"end\"] = w[\"end\"]\n","        current[\"text\"] += \" \" + w[\"word\"]\n","    else:\n","        merged_segments.append(current)\n","        current = {\"speaker\": w[\"speaker\"], \"start\": w[\"start\"], \"end\": w[\"end\"], \"text\": w[\"word\"]}\n","if current:\n","    merged_segments.append(current)\n","\n","seg_json = os.path.join(OUT_DIR, \"speaker_segments.json\")\n","with open(seg_json, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(merged_segments, f, ensure_ascii=False, indent=2)\n","\n","def format_srt_time(sec):\n","    h = int(sec // 3600); m = int((sec % 3600) // 60); s = int(sec % 60)\n","    ms = int((sec - int(sec)) * 1000)\n","    return f\"{h:02d}:{m:02d}:{s:02d},{ms:03d}\"\n","\n","srt_path = os.path.join(OUT_DIR, \"speaker_transcript.srt\")\n","with open(srt_path, \"w\", encoding=\"utf-8\") as f:\n","    for i, seg in enumerate(merged_segments, start=1):\n","        f.write(f\"{i}\\n\")\n","        f.write(f\"{format_srt_time(seg['start'])} --> {format_srt_time(seg['end'])}\\n\")\n","        f.write(f\"{seg['speaker']}: {seg['text']}\\n\\n\")\n","\n","print(\"Saved readable segments JSON:\", seg_json)\n","print(\"Saved SRT subtitles:\", srt_path)\n","\n","# ----------------- Show outputs -----------------\n","print(\"\\nüìÇ OUTPUT FILES:\")\n","for fname in sorted(os.listdir(OUT_DIR)):\n","    print(\" -\", fname)\n","print(\"\\nDone üéâ  ‚Äî download outputs from the left sidebar (folder icon) or use Colab file browser.\")\n"]},{"cell_type":"code","source":["# --- Safe audio download from YouTube ---\n","!pip install -U yt-dlp > /dev/null\n","import subprocess, shlex, os\n","\n","YOUTUBE_URL = \"https://www.youtube.com/watch?v=2Vv-BfVoq4g\"   # <-- put your URL\n","os.makedirs(\"content\", exist_ok=True)\n","AUDIO_PATH = \"content/audio.wav\"\n","VIDEO_PATH = \"content/video.mp4\"\n","\n","print(\"üîΩ Downloading YouTube audio...\")\n","try:\n","    # try direct audio extraction\n","    cmd = f\"yt-dlp -f bestaudio -x --audio-format wav -o '{AUDIO_PATH}' '{YOUTUBE_URL}'\"\n","    subprocess.run(shlex.split(cmd), check=True)\n","except subprocess.CalledProcessError:\n","    # fallback: download video then extract audio\n","    print(\"‚ö†Ô∏è  Direct WAV extraction failed, retrying with ffmpeg...\")\n","    subprocess.run(shlex.split(f\"yt-dlp -f bestaudio -o '{VIDEO_PATH}' '{YOUTUBE_URL}'\"), check=True)\n","    subprocess.run(shlex.split(f\"ffmpeg -y -i '{VIDEO_PATH}' -ar 16000 -ac 1 '{AUDIO_PATH}'\"), check=True)\n","\n","print(f\"‚úÖ Audio saved at {AUDIO_PATH}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"77-aCsPwrlbC","executionInfo":{"status":"ok","timestamp":1760545381623,"user_tz":-120,"elapsed":15642,"user":{"displayName":"Aimar Pagonabarraga Gallastegui","userId":"08004142937794377562"}},"outputId":"f501c7dd-f23e-4e9b-e70b-bf471aec409b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["üîΩ Downloading YouTube audio...\n","‚úÖ Audio saved at content/audio.wav\n"]}]},{"cell_type":"code","source":["# ============================================================\n","# COLAB SAFE PIPELINE: YouTube -> WhisperX + Pyannote (per-segment)\n","# Handles long audio without GPU crashes\n","# ============================================================\n","\n","# ----------------- Installs -----------------\n","!pip install -q --upgrade pip\n","!pip install -q -U yt-dlp\n","!pip install -q git+https://github.com/m-bain/whisperX.git\n","!pip install -q \"pyannote.audio>=2.1\" ffmpeg-python faster_whisper\n","!apt-get update -qq && apt-get install -y -qq ffmpeg\n","\n","# ----------------- Imports -----------------\n","import os, shlex, subprocess, json, csv, torch\n","from getpass import getpass\n","from pyannote.audio import Pipeline\n","import whisperx\n","\n","# ----------------- Config -----------------\n","YOUTUBE_URL = \"https://www.youtube.com/watch?v=LJiUDxj-2ZE\"  # change if desired\n","WHISPER_MODEL = \"tiny\"  # tiny = safe for Colab Free GPU\n","OUT_DIR = \"content\"\n","os.makedirs(OUT_DIR, exist_ok=True)\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","SEGMENT_TIME = 300  # segment length in seconds (5 min)\n","\n","# ----------------- Hugging Face token -----------------\n","if \"HUGGINGFACE_TOKEN\" not in os.environ:\n","    print(\"Paste your Hugging Face token (it will be hidden):\")\n","    os.environ[\"HUGGINGFACE_TOKEN\"] = getpass(\"HUGGINGFACE_TOKEN: \")\n","HUGGINGFACE_TOKEN = os.environ[\"HUGGINGFACE_TOKEN\"]\n","\n","# ----------------- Helper -----------------\n","def run(cmd):\n","    print(\">\", cmd)\n","    proc = subprocess.run(shlex.split(cmd), capture_output=True, text=True)\n","    if proc.returncode != 0:\n","        print(\"STDOUT:\", proc.stdout)\n","        print(\"STDERR:\", proc.stderr)\n","        raise subprocess.CalledProcessError(proc.returncode, cmd)\n","    return proc\n","\n","# ----------------- Download & convert audio -----------------\n","video_path = os.path.join(OUT_DIR, \"video.mp4\")\n","wav_raw = os.path.join(OUT_DIR, \"audio_raw.wav\")\n","audio_16k = os.path.join(OUT_DIR, \"audio.wav\")\n","\n","try:\n","    cmd = f\"yt-dlp -f bestaudio -x --audio-format wav -o '{wav_raw}' '{YOUTUBE_URL}'\"\n","    run(cmd)\n","except subprocess.CalledProcessError:\n","    cmd = f\"yt-dlp -f bestaudio -o '{video_path}' '{YOUTUBE_URL}'\"\n","    run(cmd)\n","    run(f\"ffmpeg -y -i '{video_path}' -ar 16000 -ac 1 -vn -acodec pcm_s16le '{audio_16k}'\")\n","else:\n","    run(f\"ffmpeg -y -i '{wav_raw}' -ar 16000 -ac 1 -vn -acodec pcm_s16le '{audio_16k}'\")\n","\n","# ----------------- Split audio into segments -----------------\n","seg_dir = os.path.join(OUT_DIR, \"segments\")\n","os.makedirs(seg_dir, exist_ok=True)\n","try:\n","    run(f\"ffmpeg -hide_banner -loglevel error -i '{audio_16k}' -f segment -segment_time {SEGMENT_TIME} -c copy {seg_dir}/segment_%03d.wav\")\n","except:\n","    run(f\"ffmpeg -y -i '{audio_16k}' -f segment -segment_time {SEGMENT_TIME} -ar 16000 -ac 1 {seg_dir}/segment_%03d.wav\")\n","\n","segments = sorted([os.path.join(seg_dir, f) for f in os.listdir(seg_dir) if f.startswith(\"segment_\")])\n","if len(segments) == 0:\n","    segments = [audio_16k]\n","print(f\"üîé {len(segments)} segment(s) to process.\")\n","\n","# ----------------- Load WhisperX -----------------\n","print(f\"üéß Loading WhisperX '{WHISPER_MODEL}' on {DEVICE} ...\")\n","model = whisperx.load_model(WHISPER_MODEL, device=DEVICE)\n","all_segments = []\n","detected_language = None\n","\n","# ----------------- Transcribe segments -----------------\n","for idx, segfile in enumerate(segments):\n","    print(f\"\\n--- Transcribing segment {idx+1}/{len(segments)} ---\")\n","    res = model.transcribe(segfile, batch_size=16)\n","    if detected_language is None:\n","        detected_language = res.get(\"language\")\n","    # align to word-level\n","    model_a, metadata = whisperx.load_align_model(language_code=res[\"language\"], device=DEVICE)\n","    aligned = whisperx.align(res[\"segments\"], model_a, metadata, segfile, device=DEVICE)\n","    # adjust start times if segment offset needed (here we assume segments are contiguous)\n","    all_segments.extend(aligned[\"segments\"])\n","\n","# Save intermediate transcription\n","whisper_json = os.path.join(OUT_DIR, \"whisperx_transcript.json\")\n","with open(whisper_json, \"w\", encoding=\"utf-8\") as f:\n","    json.dump({\"language\": detected_language, \"segments\": all_segments}, f, ensure_ascii=False, indent=2)\n","print(\"‚úÖ WhisperX transcription done.\")\n","\n","# ----------------- Diarize per segment on CPU -----------------\n","print(\"\\nüë• Running Pyannote diarization per segment on CPU...\")\n","pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=HUGGINGFACE_TOKEN, device=\"cpu\")\n","\n","from pyannote.core import Annotation, Segment as PSegment\n","combined_dia = Annotation()\n","speaker_map = {}\n","speaker_counter = 0\n","\n","for idx, segfile in enumerate(segments):\n","    print(f\"  Diarizing segment {idx+1}/{len(segments)} ...\")\n","    ann = pipeline(segfile)\n","    for turn, _, label in ann.itertracks(yield_label=True):\n","        # rename speakers to avoid collision\n","        if label not in speaker_map:\n","            speaker_map[label] = f\"S{speaker_counter:03d}\"\n","            speaker_counter += 1\n","        combined_dia[turn] = speaker_map[label]\n","\n","rttm_path = os.path.join(OUT_DIR, \"diarization.rttm\")\n","with open(rttm_path, \"w\", encoding=\"utf-8\") as f:\n","    combined_dia.write_rttm(f)\n","print(\"‚úÖ RTTM saved:\", rttm_path)\n","\n","# ----------------- Merge transcription + diarization -----------------\n","words = []\n","for seg in all_segments:\n","    if \"words\" in seg:\n","        for w in seg[\"words\"]:\n","            words.append({\n","                \"word\": w.get(\"word\") or w.get(\"text\") or \"\",\n","                \"start\": float(w.get(\"start\", seg.get(\"start\", 0))),\n","                \"end\": float(w.get(\"end\", seg.get(\"end\", 0)))\n","            })\n","    else:\n","        words.append({\n","            \"word\": seg.get(\"text\", \"\").strip(),\n","            \"start\": float(seg.get(\"start\", 0)),\n","            \"end\": float(seg.get(\"end\", 0))\n","        })\n","\n","# assign speakers\n","dia_segments = [{\"start\": t.start, \"end\": t.end, \"speaker\": l} for t, _, l in combined_dia.itertracks(yield_label=True)]\n","def assign_speaker(w_start, w_end):\n","    best_speaker, best_overlap = \"SPEAKER_UNKNOWN\", 0\n","    for seg in dia_segments:\n","        overlap = max(0, min(w_end, seg[\"end\"]) - max(w_start, seg[\"start\"]))\n","        if overlap > best_overlap:\n","            best_overlap = overlap\n","            best_speaker = seg[\"speaker\"]\n","    return best_speaker\n","\n","for w in words:\n","    w[\"speaker\"] = assign_speaker(w[\"start\"], w[\"end\"])\n","\n","# save merged outputs\n","merged_json = os.path.join(OUT_DIR, \"merged_words_with_speakers.json\")\n","with open(merged_json, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(words, f, ensure_ascii=False, indent=2)\n","\n","csv_path = os.path.join(OUT_DIR, \"merged_words_with_speakers.csv\")\n","with open(csv_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n","    writer = csv.DictWriter(f, fieldnames=[\"start\", \"end\", \"speaker\", \"word\"])\n","    writer.writeheader()\n","    for w in words:\n","        writer.writerow(w)\n","\n","# ----------------- Merge words into speaker segments & SRT -----------------\n","merged_segments = []\n","current = None\n","for w in words:\n","    if current is None:\n","        current = {\"speaker\": w[\"speaker\"], \"start\": w[\"start\"], \"end\": w[\"end\"], \"text\": w[\"word\"]}\n","    elif w[\"speaker\"] == current[\"speaker\"] and w[\"start\"] - current[\"end\"] <= 1.0:\n","        current[\"end\"] = w[\"end\"]\n","        current[\"text\"] += \" \" + w[\"word\"]\n","    else:\n","        merged_segments.append(current)\n","        current = {\"speaker\": w[\"speaker\"], \"start\": w[\"start\"], \"end\": w[\"end\"], \"text\": w[\"word\"]}\n","if current:\n","    merged_segments.append(current)\n","\n","seg_json = os.path.join(OUT_DIR, \"speaker_segments.json\")\n","with open(seg_json, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(merged_segments, f, ensure_ascii=False, indent=2)\n","\n","def format_srt_time(sec):\n","    h = int(sec // 3600); m = int((sec % 3600) // 60); s = int(sec % 60)\n","    ms = int((sec - int(sec)) * 1000)\n","    return f\"{h:02d}:{m:02d}:{s:02d},{ms:03d}\"\n","\n","srt_path = os.path.join(OUT_DIR, \"speaker_transcript.srt\")\n","with open(srt_path, \"w\", encoding=\"utf-8\") as f:\n","    for i, seg in enumerate(merged_segments, start=1):\n","        f.write(f\"{i}\\n\")\n","        f.write(f\"{format_srt_time(seg['start'])} --> {format_srt_time(seg['end'])}\\n\")\n","        f.write(f\"{seg['speaker']}: {seg['text']}\\n\\n\")\n","\n","# ----------------- Show outputs -----------------\n","print(\"\\nüìÇ OUTPUT FILES:\")\n","for f in os.listdir(OUT_DIR):\n","    print(\" -\", f)\n","print(\"\\nAll done! üéâ\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3D9qGxzmt9d3","outputId":"36dcbe53-e7db-4160-a22c-eb54b7385674"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/pyannote/audio/core/io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n","  torchaudio.list_audio_backends()\n"]},{"output_type":"stream","name":"stdout","text":["Paste your Hugging Face token (it will be hidden):\n","HUGGINGFACE_TOKEN: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n","> yt-dlp -f bestaudio -x --audio-format wav -o 'content/audio_raw.wav' 'https://www.youtube.com/watch?v=LJiUDxj-2ZE'\n","> ffmpeg -y -i 'content/audio_raw.wav' -ar 16000 -ac 1 -vn -acodec pcm_s16le 'content/audio.wav'\n","> ffmpeg -hide_banner -loglevel error -i 'content/audio.wav' -f segment -segment_time 300 -c copy content/segments/segment_%03d.wav\n","üîé 9 segment(s) to process.\n","üéß Loading WhisperX 'tiny' on cuda ...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/speechbrain/utils/torch_audio_backend.py:57: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n","  available_backends = torchaudio.list_audio_backends()\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _speechbrain_save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _speechbrain_load\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for load\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint save hook for _save\n","DEBUG:speechbrain.utils.checkpoints:Registered checkpoint load hook for _recover\n","/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["2025-10-15 16:35:07 - whisperx.asr - INFO - No language specified, language will be detected for each audio file (increases inference time)\n","2025-10-15 16:35:07 - whisperx.vads.pyannote - INFO - Performing voice activity detection using Pyannote...\n"]},{"output_type":"stream","name":"stderr","text":["INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.5. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../usr/local/lib/python3.12/dist-packages/whisperx/assets/pytorch_model.bin`\n"]},{"output_type":"stream","name":"stdout","text":["Model was trained with pyannote.audio 0.0.1, yours is 3.4.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n","Model was trained with torch 1.10.0+cu102, yours is 2.8.0+cu126. Bad things might happen unless you revert torch to 1.x.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/pyannote/audio/core/io.py:212: UserWarning: torchaudio._backend.list_audio_backends has been deprecated. This deprecation is part of a large refactoring effort to transition TorchAudio into a maintenance phase. The decoding and encoding capabilities of PyTorch for both audio and video are being consolidated into TorchCodec. Please see https://github.com/pytorch/audio/issues/3902 for more information. It will be removed from the 2.9 release. \n","  torchaudio.list_audio_backends()\n"]},{"output_type":"stream","name":"stdout","text":["\n","--- Transcribing segment 1/9 ---\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/pyannote/audio/utils/reproducibility.py:74: ReproducibilityWarning: TensorFloat-32 (TF32) has been disabled as it might lead to reproducibility issues and lower accuracy.\n","It can be re-enabled by calling\n","   >>> import torch\n","   >>> torch.backends.cuda.matmul.allow_tf32 = True\n","   >>> torch.backends.cudnn.allow_tf32 = True\n","See https://github.com/pyannote/pyannote-audio/issues/1370 for more details.\n","\n","  warnings.warn(\n"]}]}]}