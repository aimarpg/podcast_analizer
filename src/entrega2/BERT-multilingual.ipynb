{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMm4X0Ou6x52hGrC7GjHFs+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","\n","BASE_DIR = \"/content/drive/Shareddrives/NLP/transcriptions\"\n","TEXT_DIR = os.path.join(BASE_DIR, \"all_transcriptions\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fVJH9B0dUS_v","executionInfo":{"status":"ok","timestamp":1761764174681,"user_tz":-60,"elapsed":18665,"user":{"displayName":"Leire Larrauri Olarte","userId":"15613160182125551834"}},"outputId":"0acf8e27-e9ec-485a-e194-4dc360c3bf25"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install spacy\n","!python -m spacy download es_core_news_sm\n","\n","import os\n","import glob\n","import csv\n","import re\n","import unicodedata\n","import spacy\n","\n","\n","OUTPUT_DIR  = r\"/content/output/preprocessing_steps\"\n","SPACY_MODEL = \"es_core_news_sm\"\n","REMOVE_STOPWORDS = True\n","\n","TOKEN_PATTERN = re.compile(r\"^[A-Za-zÀ-ÖØ-öø-ÿ]{2,}$\")\n","\n","def normalize_text(text: str) -> str:\n","    \"\"\"Unicode normalize + lowercase (case folding).\"\"\"\n","    return unicodedata.normalize(\"NFKC\", text).lower()\n","\n","def ensure_dir(path: str):\n","    os.makedirs(path, exist_ok=True)\n","\n","def main():\n","    ensure_dir(OUTPUT_DIR)\n","    print(f\"Cargando modelo spaCy: {SPACY_MODEL}\")\n","    nlp = spacy.load(SPACY_MODEL, disable=[\"ner\"])\n","\n","    files = sorted(glob.glob(os.path.join(TEXT_DIR, \"*.txt\")))\n","    if not files:\n","        raise FileNotFoundError(f\"No se encuentran .txt en: {TEXT_DIR}\")\n","\n","    index_rows = []\n","    for path in files:\n","        base = os.path.splitext(os.path.basename(path))[0]\n","        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n","            text = f.read()\n","\n","        doc = nlp(normalize_text(text))\n","\n","        raw_tokens, lemmas = [], []\n","        for tok in doc:\n","            if tok.is_space:\n","                continue\n","            if not TOKEN_PATTERN.match(tok.text):\n","                continue\n","            if REMOVE_STOPWORDS and tok.is_stop:\n","                continue\n","            raw_tokens.append(tok.text)\n","            lemmas.append(tok.lemma_ if tok.lemma_ else tok.text)\n","\n","        # --- Save cleaned outputs ---\n","        with open(os.path.join(OUTPUT_DIR, f\"{base}__clean_raw.txt\"), \"w\", encoding=\"utf-8\") as f:\n","            f.write(\" \".join(raw_tokens))\n","        with open(os.path.join(OUTPUT_DIR, f\"{base}__clean_lemma.txt\"), \"w\", encoding=\"utf-8\") as f:\n","            f.write(\" \".join(lemmas))\n","\n","        # Per-document CSV (raw vs lemma)\n","        per_doc_csv = os.path.join(OUTPUT_DIR, f\"{base}__tokens.csv\")\n","        max_len = max(len(raw_tokens), len(lemmas))\n","        with open(per_doc_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n","            w = csv.writer(f)\n","            w.writerow([\"raw\", \"lemma\"])\n","            for i in range(max_len):\n","                w.writerow([\n","                    raw_tokens[i] if i < len(raw_tokens) else \"\",\n","                    lemmas[i] if i < len(lemmas) else \"\"\n","                ])\n","\n","        index_rows.append([os.path.basename(path), len(raw_tokens), len(set(raw_tokens))])\n","        print(f\"Procesado: {os.path.basename(path)}  (tokens: {len(raw_tokens)}, únicos: {len(set(raw_tokens))})\")\n","\n","    # Corpus index\n","    with open(os.path.join(OUTPUT_DIR, \"_corpus_index.csv\"), \"w\", newline=\"\", encoding=\"utf-8\") as f:\n","        w = csv.writer(f)\n","        w.writerow([\"file\", \"token_count\", \"unique_token_count\"])\n","        w.writerows(index_rows)\n","\n","    print(\"\\nListo. Salidas en:\", OUTPUT_DIR)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3dNMjDx4UUou","executionInfo":{"status":"ok","timestamp":1761764289646,"user_tz":-60,"elapsed":111630,"user":{"displayName":"Leire Larrauri Olarte","userId":"15613160182125551834"}},"outputId":"86d5c41b-16d8-448e-dc23-d6ac436464a0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n","Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n","Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n","Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n","Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n","Collecting es-core-news-sm==3.8.0\n","  Using cached https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('es_core_news_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n","Cargando modelo spaCy: es_core_news_sm\n","Procesado: 11 Hábitos Diarios para tener una Salud del 1% (Fuerza Explosiva).txt  (tokens: 6047, únicos: 2132)\n","Procesado: 15 Hábitos para Vivir con Abundancia y Tener Éxito (Sergio Fernández).txt  (tokens: 5447, únicos: 2079)\n","Procesado: 299 DÍAS SECUESTRADO, ¿cómo es ser REPORTERO DE GUERRA_ - Tenía la Duda 2x01.txt  (tokens: 1674, únicos: 981)\n","Procesado: ABOGADOS TUMBAN los BULOS sobre EXTRANJERÍA.txt  (tokens: 5175, únicos: 2660)\n","Procesado: ARANCELES_ TRUMP REVIENTA SU PROPIA BOLSA.txt  (tokens: 2323, únicos: 1178)\n","Procesado: Analizamos el ACUERDO COMERCIAL entre la UE y EEUU.txt  (tokens: 2755, únicos: 1282)\n","Procesado: Armados pero bien mandados.txt  (tokens: 2843, únicos: 1518)\n","Procesado: Así es TRABAJAR de TANATOPRACTORA_ ¿Maquilladora de muertos.txt  (tokens: 1033, únicos: 633)\n","Procesado: Así es VIVIR CON AUTISMO_ Lo que no sabíamos _ Tenía la Duda 3x08.txt  (tokens: 990, únicos: 610)\n","Procesado: Así es VIVIR de ATRACAR BANCOS_ Lo que las películas no enseñan.txt  (tokens: 1103, únicos: 680)\n","Procesado: Así es la vida de una MONJA DE CLAUSURA _ Tenía La Duda 1x02.txt  (tokens: 1251, únicos: 700)\n","Procesado: Así es ser MUJER CULTURISTA_ ¿cuánto ganan y cómo entrenan_ ¿usan sustancias.txt  (tokens: 965, únicos: 578)\n","Procesado: Así es vivir con SÍNDROME DE DOWN_ explicado en primera persona.txt  (tokens: 828, únicos: 506)\n","Procesado: Así es vivir con TOC_ explicado en primera persona _ Tenía la Duda 5x07.txt  (tokens: 825, únicos: 462)\n","Procesado: Así se resolvió el primer crimen de la historia _ Tenía La Duda 1x01.txt  (tokens: 1089, únicos: 729)\n","Procesado: ÁFRICA y las MATERIAS PRIMAS.txt  (tokens: 1072, únicos: 596)\n","Procesado: CAYENDO por su propio PESO.txt  (tokens: 1827, únicos: 981)\n","Procesado: CÁRCEL por PROTESTAR (con Francho AIJÓN).txt  (tokens: 3165, únicos: 1507)\n","Procesado: COMUNICACIÓN POLÍTICA y HEGEMONÍA.txt  (tokens: 3184, únicos: 1650)\n","Procesado: Caso MONTORO_ CÓMO FUNCIONAN las CLOACAS.txt  (tokens: 2299, únicos: 1203)\n","Procesado: Cómo Escapar del Sistema y Vivir la Vida que sueñas (Llados Fitness).txt  (tokens: 6914, únicos: 2275)\n","Procesado: Cómo Ganar +10.000€ al Mes con 17 Años (DollarDorado).txt  (tokens: 4316, únicos: 1634)\n","Procesado: Cómo Hizo su Empresa de 1.000.000€ con Sólo 22 Años (Gala Freixa).txt  (tokens: 2395, únicos: 1165)\n","Procesado: Cómo Influenciar al Mundo con tu Comunicación (Fer Miralles).txt  (tokens: 7053, únicos: 2960)\n","Procesado: Cómo Ser Libre con la Inversión Inmobiliaria con Poco Dinero (Carlos Galán).txt  (tokens: 4255, únicos: 1621)\n","Procesado: Cómo Ser un Líder y un Emprendedor de Éxito (Powerexplosive).txt  (tokens: 6787, únicos: 2394)\n","Procesado: Cómo Superar una Ruptura de Amor y Ser tu mejor Versión.txt  (tokens: 5136, únicos: 2055)\n","Procesado: Cómo Tener una Mentalidad de Éxito para Ser el Mejor (Joan Pradells).txt  (tokens: 4980, únicos: 1860)\n","Procesado: Cómo Vender casi Cualquier Cosa Online (Experto Facebook Ads).txt  (tokens: 5017, únicos: 1991)\n","Procesado: Cómo la DERECHA ALTERNATIVA se convirtió en MAINSTREAM.txt  (tokens: 1966, únicos: 1061)\n","Procesado: Cómo ser una persona del top 1% mundial (Adrià Solà Pastor).txt  (tokens: 6614, únicos: 2751)\n","Procesado: Cómo tener el Mejor Sexo de tu Vida y sus Beneficios (sexóloga).txt  (tokens: 4412, únicos: 1721)\n","Procesado: Crimen y castigo – Debate Directo 19-3-2018.txt  (tokens: 4669, únicos: 2311)\n","Procesado: DOMINATRIX PROFESIONAL responde VUESTRAS PREGUNTAS _ Tenía la Duda 1x09.txt  (tokens: 841, únicos: 556)\n","Procesado: De 0€ a 16.530€ con el Podcast en 5 Meses (Especial 100k).txt  (tokens: 5445, únicos: 2135)\n","Procesado: De Cocinar en la Calle a Conquistar Nueva York (Chef Jose Luis).txt  (tokens: 3961, únicos: 1787)\n","Procesado: De Vivir en un Almacén a 3.000.000€ en un Mes (Preico Juridicos).txt  (tokens: 6137, únicos: 2567)\n","Procesado: De la OBRA a 1.000.000€ con Amazon en 3 Años (Ruben AMZ).txt  (tokens: 4754, únicos: 1812)\n","Procesado: ENTENDIENDO la OFERTA de RUSIA.txt  (tokens: 1332, únicos: 758)\n","Procesado: ENVÍA BUQUES de GUERRA y DESTRUCTORES a VENEZUELA.txt  (tokens: 1256, únicos: 721)\n","Procesado: ESPECIAL_ SIRIA AFECTARÁ a TODA la GEOPOLÍTICA.txt  (tokens: 4864, únicos: 2178)\n","Procesado: EUROPA FLIRTEA CON LA ULTRADERECHA.txt  (tokens: 1821, únicos: 970)\n","Procesado: EUROPA_ del verde ECOLOGISTA al verde MILITAR.txt  (tokens: 2927, únicos: 1551)\n","Procesado: El BRAZO ARMADO de TRUMP.txt  (tokens: 2674, únicos: 1360)\n","Procesado: El PLAN de EUROPA para la DERECHA.txt  (tokens: 1866, únicos: 944)\n","Procesado: El PLAN de ILLA para CATALUÑA.txt  (tokens: 1051, únicos: 606)\n","Procesado: El SECRETO mejor guardado del FRIGOPIÉ _ Tenía la duda 1x04.txt  (tokens: 746, únicos: 512)\n","Procesado: El SÍNTOMA de un ENORME PROBLEMA POLÍTICO.txt  (tokens: 1803, únicos: 1002)\n","Procesado: El _TODOS son IGUALES_ y la _ANTIPOLÍTICA.txt  (tokens: 2420, únicos: 1344)\n","Procesado: España, SAHARA y Marruecos_ las CLAVES del CONFLICTO con TALEB ALISALEM.txt  (tokens: 3446, únicos: 1502)\n","Procesado: FEMINISMO DE CLASE MEDIA.txt  (tokens: 3189, únicos: 1464)\n","Procesado: Gana 65.000€ al Mes con 17 Años (Anas Escríbelo.ai).txt  (tokens: 6624, únicos: 2384)\n","Procesado: HUELGA en la EDUCACIÓN PÚBLICA.txt  (tokens: 1711, únicos: 940)\n","Procesado: Ha Creado una Empresa de 40 Millones con 25 años (Internxt).txt  (tokens: 5507, únicos: 2073)\n","Procesado: Habla la ABOGADA de las SEIS de la SUIZA.txt  (tokens: 1450, únicos: 768)\n","Procesado: INCENDIOS DE VERANO con el INGENIERO TÉCNICO AGRÍCOLA Felipe Marín.txt  (tokens: 5369, únicos: 2242)\n","Procesado: INTELIGENCIA ARTIFICIAL_ prepararse para su LLEGADA.txt  (tokens: 1971, únicos: 1038)\n","Procesado: Ingeniero de REDES ELÉCTRICAS HABLA sobre el APAGÓN.txt  (tokens: 3267, únicos: 1480)\n","Procesado: La HIPNOSIS_ finalmente BIEN EXPLICADA por un EXPERTO _ Tenía la Duda.txt  (tokens: 1153, únicos: 667)\n","Procesado: La INQUIETANTE HISTORIA de COCA-COLA _ Tenía la duda 1x05.txt  (tokens: 1202, únicos: 712)\n","Procesado: La REALIDAD de TRABAJAR en un CIRCO_ Los Payasos de la Tele _ Tenía la Duda 4x07.txt  (tokens: 892, únicos: 566)\n","Procesado: La RECONCILIACIÓN que NUNCA FUE.txt  (tokens: 3031, únicos: 1716)\n","Procesado: La situación en EEUU es MUY CERCANA a la RUPTURA.txt  (tokens: 3124, únicos: 1796)\n","Procesado: Las CLAVES del DISCURSO de ALVISE.txt  (tokens: 1846, únicos: 1010)\n","Procesado: Las MEDIDAS que SÍ BAJARÍAN el PRECIO de la VIVIENDA.txt  (tokens: 1655, únicos: 879)\n","Procesado: Los 6 de ZARAGOZA_ ENTREVISTA con FRANCHO AIJÓN, PADRE de uno de los ENCARCELADOS.txt  (tokens: 2101, únicos: 1170)\n","Procesado: Los DINOSAURIOS TODAVÍA EXISTEN_ Paleontólogo explica dónde.txt  (tokens: 992, únicos: 663)\n","Procesado: Los SECRETOS mejor guardados del ANTIGUO EGIPTO _ Tenía la duda 1x08 @Historiaen5minutos.txt  (tokens: 936, únicos: 636)\n","Procesado: Los _GROYPERS_, ¿la IDEOLOGÍA detrás del ASESINO de CHARLIE KIRK.txt  (tokens: 2170, únicos: 1168)\n","Procesado: Los extraterrestres EXISTEN, ¿por qué no los hemos visto todavía_ _ Tenía la Duda 1x10.txt  (tokens: 1719, únicos: 885)\n","Procesado: Lucha IDEOLÓGICA en el VATICANO.txt  (tokens: 1810, únicos: 1012)\n","Procesado: Mayor SMI y reducción de jornada_ ¿se queda corto.txt  (tokens: 1315, únicos: 700)\n","Procesado: Mucho RUIDO y PÉSIMA GESTIÓN.txt  (tokens: 1577, únicos: 842)\n","Procesado: Nueva POLÍTICA de DEFENSA en EEUU.txt  (tokens: 1580, únicos: 848)\n","Procesado: PERDIENDO con el DISCURSO de la DERECHA.txt  (tokens: 1574, únicos: 951)\n","Procesado: POR QUÉ NOS ENAMORAMOS de una persona y NO de otra.txt  (tokens: 681, únicos: 400)\n","Procesado: Por qué SÍ HAY QUE HABLAR DE POLÍTICA en el APAGÓN.txt  (tokens: 2418, únicos: 1380)\n","Procesado: Por qué el 97% de la Población No es Feliz realmente (Psiquiatra Dr. de la Gándara).txt  (tokens: 6791, únicos: 2534)\n","Procesado: Por qué el 99% de las Emprendedores Fracasan y Cómo Empezar.txt  (tokens: 3398, únicos: 1483)\n","Procesado: Por qué los Chinos Emprenden sólo Bazar o Wok en España.txt  (tokens: 2989, únicos: 1339)\n","Procesado: Pregunté a un Millonario el Método para Ganar 1.000.000€ (Hermo Benito).txt  (tokens: 4962, únicos: 1944)\n","Procesado: QUÉ HAY EN JUEGO en la FUSIÓN del BBVA y el SABADELL.txt  (tokens: 1901, únicos: 994)\n","Procesado: RETIRADA en el PRIMER ASALTO.txt  (tokens: 1933, únicos: 1083)\n","Procesado: Republicanismo contra el AUTORITARISMO que VIENE.txt  (tokens: 2253, únicos: 1065)\n","Procesado: Se ACABÓ el EXPERIMENTO (salió MAL).txt  (tokens: 2986, únicos: 1459)\n","Procesado: Situación INSOSTENIBLE en GAZA.txt  (tokens: 1513, únicos: 907)\n","Procesado: TRUMP y la ILUSTRACIÓN OSCURA.txt  (tokens: 1118, únicos: 609)\n","Procesado: Torre Pacheco es una ADVERTENCIA.txt  (tokens: 3005, únicos: 1404)\n","Procesado: Trump AMENAZA a ESPAÑA.txt  (tokens: 1823, únicos: 969)\n","Procesado: Un MANUAL para MANIPULAR las DEMOCRACIAS.txt  (tokens: 770, únicos: 497)\n","Procesado: y la SEGUNDA OLEADA de NUEVA IZQUIERDA.txt  (tokens: 3648, únicos: 1674)\n","Procesado: ¿Cuánta PRESIÓN puede AGUANTAR Israel.txt  (tokens: 1884, únicos: 1030)\n","Procesado: ¿DIMITE HOY Pedro Sánchez.txt  (tokens: 5022, únicos: 2161)\n","Procesado: ¿GIRO a la IZQUIERDA de Pedro SÁNCHEZ.txt  (tokens: 1860, únicos: 976)\n","Procesado: ¿Hasta dónde puede llegar el Gobierno.txt  (tokens: 2015, únicos: 1009)\n","Procesado: ¿MAGA contra SILICON VALLEY.txt  (tokens: 1893, únicos: 1064)\n","Procesado: ¿Por qué dejaron de CAMINAR los DELFINES_ _ Tenía la Duda 1x03.txt  (tokens: 1260, únicos: 829)\n","Procesado: ¿Por qué el 666 es el NÚMERO DE SATANÁS_ La verdadera historia _ Tenía la Duda 2x06.txt  (tokens: 1184, únicos: 774)\n","Procesado: ¿Puede REMONTAR la IZQUIERDA.txt  (tokens: 1693, únicos: 915)\n","Procesado: ¿Puede provocar TRUMP una GUERRA MUNDIAL.txt  (tokens: 1764, únicos: 929)\n","Procesado: ¿QUÉ SENTIMOS AL MORIR_ ¿Y de qué nos solemos arrepentir.txt  (tokens: 1037, únicos: 610)\n","Procesado: ¿Qué SECRETOS ocultaban los PRIMEROS HUMANOS_ Así eran LOS NEANDERTALES.txt  (tokens: 970, únicos: 594)\n","Procesado: ¿Qué busca el Gobierno de TRUMP.txt  (tokens: 1852, únicos: 1031)\n","Procesado: ¿Qué puede hacer Europa ante TRUMP y VANCE.txt  (tokens: 1882, únicos: 1104)\n","\n","Listo. Salidas en: /content/output/preprocessing_steps\n"]}]},{"cell_type":"markdown","source":["BERT Multilingual"],"metadata":{"id":"pQG1xLMKUl2N"}},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hBh_Ut6oT9si","executionInfo":{"status":"ok","timestamp":1761764765264,"user_tz":-60,"elapsed":16367,"user":{"displayName":"Leire Larrauri Olarte","userId":"15613160182125551834"}},"outputId":"915bad4b-dc81-4493-a0c5-f13d20ee1d74"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Using device: cuda\n","Multilingual BERT model loaded successfully!\n","Processing transcripts from: /content/drive/Shareddrives/NLP/transcriptions/all_transcriptions\n","Starting Multilingual BERT embedding generation...\n","Found 104 transcript files\n"]},{"output_type":"stream","name":"stderr","text":["Processing files: 100%|██████████| 104/104 [00:14<00:00,  7.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Embeddings saved to /content/drive/Shareddrives/NLP/transcriptions/bert_multilingual_embeddings.csv\n","\n","Embedding generation completed!\n","Processed 104 files\n","Embedding dimension: 768\n","First embedding shape: (768,)\n","First embedding sample (first 10 dims): [-0.47443473 -0.30613977  0.16426256  0.18417338  0.23747234 -0.10219991\n","  0.02824556 -0.39680415  0.00480094  0.17787099]\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import torch\n","from transformers import AutoTokenizer, AutoModel\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Set base directories\n","BASE_DIR = \"/content/drive/Shareddrives/NLP/transcriptions\"\n","TEXT_DIR = os.path.join(BASE_DIR, \"all_transcriptions\")\n","\n","# Load Multilingual BERT model and tokenizer\n","model_name = \"bert-base-multilingual-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModel.from_pretrained(model_name)\n","model.to(device)\n","model.eval()  # Set to evaluation mode\n","\n","print(\"Multilingual BERT model loaded successfully!\")\n","\n","def get_embeddings(text, method=\"mean\"):\n","    \"\"\"\n","    Get contextual embeddings for a text using Multilingual BERT\n","\n","    Args:\n","        text (str): Input text\n","        method (str): How to aggregate token embeddings\n","                     \"mean\" - mean of all token embeddings\n","                     \"cls\" - use [CLS] token embedding\n","                     \"pooler\" - use pooler output\n","\n","    Returns:\n","        numpy array: Embedding vector\n","    \"\"\"\n","    # Tokenize the text\n","    inputs = tokenizer(\n","        text,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        padding=True,\n","        max_length=512,  # BERT's maximum length\n","        return_attention_mask=True\n","    )\n","\n","    # Move to device\n","    inputs = {key: value.to(device) for key, value in inputs.items()}\n","\n","    # Get embeddings without calculating gradients\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","\n","    # Choose embedding aggregation method\n","    if method == \"cls\":\n","        # Use [CLS] token embedding\n","        embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n","    elif method == \"mean\":\n","        # Mean of all token embeddings (excluding padding tokens)\n","        attention_mask = inputs['attention_mask']\n","        token_embeddings = outputs.last_hidden_state\n","\n","        # Create mask for non-padding tokens\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","\n","        # Sum embeddings and divide by number of non-padding tokens\n","        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n","        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","        embedding = (sum_embeddings / sum_mask).cpu().numpy()\n","    elif method == \"pooler\":\n","        # Use pooler output\n","        embedding = outputs.pooler_output.cpu().numpy()\n","    else:\n","        raise ValueError(\"Method must be 'mean', 'cls', or 'pooler'\")\n","\n","    return embedding.squeeze()\n","\n","def process_transcript_files(text_dir, output_file=\"bert_multilingual_embeddings.csv\"):\n","    \"\"\"\n","    Process all transcript files and generate Multilingual BERT embeddings\n","\n","    Args:\n","        text_dir (str): Directory containing transcript files\n","        output_file (str): Name of output CSV file\n","    \"\"\"\n","    # Get all text files\n","    text_files = [f for f in os.listdir(text_dir) if f.endswith('.txt')]\n","    print(f\"Found {len(text_files)} transcript files\")\n","\n","    embeddings_data = []\n","\n","    for filename in tqdm(text_files, desc=\"Processing files\"):\n","        file_path = os.path.join(text_dir, filename)\n","\n","        try:\n","            # Read the transcript\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                transcript = f.read().strip()\n","\n","            if not transcript:\n","                print(f\"Warning: Empty file {filename}\")\n","                continue\n","\n","            # Get embeddings using different methods\n","            mean_embedding = get_embeddings(transcript, method=\"mean\")\n","            cls_embedding = get_embeddings(transcript, method=\"cls\")\n","\n","            # Store results\n","            embeddings_data.append({\n","                'filename': filename,\n","                'text_length': len(transcript),\n","                'mean_embedding': mean_embedding.tolist(),\n","                'cls_embedding': cls_embedding.tolist(),\n","                'embedding_dim': len(mean_embedding)\n","            })\n","\n","        except Exception as e:\n","            print(f\"Error processing {filename}: {str(e)}\")\n","            continue\n","\n","    # Create DataFrame and save\n","    df = pd.DataFrame(embeddings_data)\n","\n","    # Save to CSV in the BASE_DIR\n","    output_path = os.path.join(BASE_DIR, output_file)\n","    df.to_csv(output_path, index=False)\n","    print(f\"Embeddings saved to {output_path}\")\n","\n","    return df\n","\n","# Function to load and visualize embeddings\n","def load_and_analyze_embeddings(csv_path):\n","    \"\"\"\n","    Load saved embeddings and provide basic analysis\n","    \"\"\"\n","    df = pd.read_csv(csv_path)\n","\n","    print(\"Embeddings DataFrame Info:\")\n","    print(f\"Shape: {df.shape}\")\n","    print(f\"Columns: {df.columns.tolist()}\")\n","    print(f\"Embedding dimension: {df['embedding_dim'].iloc[0]}\")\n","\n","    # Convert string representations back to numpy arrays\n","    df['mean_embedding_array'] = df['mean_embedding'].apply(\n","        lambda x: np.array(eval(x)) if isinstance(x, str) else np.array(x)\n","    )\n","    df['cls_embedding_array'] = df['cls_embedding'].apply(\n","        lambda x: np.array(eval(x)) if isinstance(x, str) else np.array(x)\n","    )\n","\n","    return df\n","\n","# Main execution\n","if __name__ == \"__main__\":\n","    # Check if directory exists\n","    if not os.path.exists(TEXT_DIR):\n","        print(f\"Error: Directory {TEXT_DIR} does not exist!\")\n","        print(\"Please check your Google Drive mounting and path\")\n","    else:\n","        print(f\"Processing transcripts from: {TEXT_DIR}\")\n","\n","        # Process all transcript files\n","        print(\"Starting Multilingual BERT embedding generation...\")\n","        embeddings_df = process_transcript_files(TEXT_DIR)\n","\n","        # Display basic info\n","        print(\"\\nEmbedding generation completed!\")\n","        print(f\"Processed {len(embeddings_df)} files\")\n","\n","        if len(embeddings_df) > 0:\n","            print(f\"Embedding dimension: {embeddings_df['embedding_dim'].iloc[0]}\")\n","\n","            # CORRECCIÓN: Verificar el tipo de dato antes de convertir\n","            first_embedding_value = embeddings_df['mean_embedding'].iloc[0]\n","\n","            if isinstance(first_embedding_value, str):\n","                # Si es string, usar eval\n","                first_embedding = np.array(eval(first_embedding_value))\n","            else:\n","                # Si ya es lista, convertir directamente\n","                first_embedding = np.array(first_embedding_value)\n","\n","            print(f\"First embedding shape: {first_embedding.shape}\")\n","            print(f\"First embedding sample (first 10 dims): {first_embedding[:10]}\")"]},{"cell_type":"code","source":["Fine-tuned"],"metadata":{"id":"g1ZeF76xUp58"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, AutoModelForMaskedLM, get_linear_schedule_with_warmup\n","from torch.optim import AdamW\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import random\n","\n","class TranscriptDataset(Dataset):\n","    def __init__(self, texts, tokenizer, max_length=512):\n","        self.texts = texts\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts[idx])\n","\n","        # Tokenize the text\n","        encoding = self.tokenizer(\n","            text,\n","            truncation=True,\n","            padding='max_length',\n","            max_length=self.max_length,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten()\n","        }\n","\n","def prepare_fine_tuning_data(text_dir, train_ratio=0.8):\n","    \"\"\"Prepare data for fine-tuning from transcript files\"\"\"\n","    text_files = [f for f in os.listdir(text_dir) if f.endswith('.txt')]\n","    texts = []\n","\n","    for filename in text_files:\n","        file_path = os.path.join(text_dir, filename)\n","        try:\n","            with open(file_path, 'r', encoding='utf-8') as f:\n","                transcript = f.read().strip()\n","            if transcript:\n","                texts.append(transcript)\n","        except Exception as e:\n","            print(f\"Error reading {filename}: {e}\")\n","\n","    # Shuffle and split\n","    random.shuffle(texts)\n","    split_idx = int(len(texts) * train_ratio)\n","    train_texts = texts[:split_idx]\n","    val_texts = texts[split_idx:]\n","\n","    return train_texts, val_texts\n","\n","def fine_tune_mlm(model, tokenizer, train_texts, val_texts, output_dir,\n","                  batch_size=8, epochs=3, learning_rate=2e-5):\n","    \"\"\"\n","    Fine-tune BERT Multilingual using Masked Language Modeling\n","    \"\"\"\n","    # Create datasets\n","    train_dataset = TranscriptDataset(train_texts, tokenizer)\n","    val_dataset = TranscriptDataset(val_texts, tokenizer)\n","\n","    # Create data loaders\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","    # Setup optimizer and scheduler\n","    optimizer = AdamW(model.parameters(), lr=learning_rate)\n","    total_steps = len(train_loader) * epochs\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer,\n","        num_warmup_steps=0,\n","        num_training_steps=total_steps\n","    )\n","\n","    # Training loop\n","    model.train()\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    for epoch in range(epochs):\n","        print(f\"Epoch {epoch + 1}/{epochs}\")\n","        total_loss = 0\n","\n","        for batch in tqdm(train_loader, desc=\"Training\"):\n","            # Move batch to device\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","\n","            # Prepare inputs for MLM - randomly mask tokens\n","            inputs = input_ids.clone()\n","            labels = input_ids.clone()\n","\n","            # Create random mask (15% of tokens)\n","            probability_matrix = torch.full(labels.shape, 0.15)\n","            masked_indices = torch.bernoulli(probability_matrix).bool()\n","            labels[~masked_indices] = -100  # Only compute loss on masked tokens\n","\n","            # 80% of the time, replace masked tokens with [MASK]\n","            indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n","            inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n","\n","            # 10% of the time, replace masked tokens with random word\n","            indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n","            random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long).to(device)\n","            inputs[indices_random] = random_words[indices_random]\n","\n","            # Forward pass\n","            outputs = model(input_ids=inputs, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","\n","            # Backward pass\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","            optimizer.zero_grad()\n","\n","            total_loss += loss.item()\n","\n","        avg_loss = total_loss / len(train_loader)\n","        print(f\"Average training loss: {avg_loss:.4f}\")\n","\n","        # Validation\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for batch in tqdm(val_loader, desc=\"Validation\"):\n","                input_ids = batch['input_ids'].to(device)\n","                attention_mask = batch['attention_mask'].to(device)\n","\n","                # Prepare inputs for MLM\n","                inputs = input_ids.clone()\n","                labels = input_ids.clone()\n","                probability_matrix = torch.full(labels.shape, 0.15)\n","                masked_indices = torch.bernoulli(probability_matrix).bool()\n","                labels[~masked_indices] = -100\n","\n","                outputs = model(input_ids=inputs, attention_mask=attention_mask, labels=labels)\n","                val_loss += outputs.loss.item()\n","\n","        avg_val_loss = val_loss / len(val_loader)\n","        print(f\"Average validation loss: {avg_val_loss:.4f}\")\n","        model.train()\n","\n","    # Save the fine-tuned model\n","    model.save_pretrained(output_dir)\n","    tokenizer.save_pretrained(output_dir)\n","    print(f\"Fine-tuned model saved to {output_dir}\")\n","\n","def get_fine_tuned_embeddings(text, model, tokenizer, method=\"mean\"):\n","    \"\"\"\n","    Get embeddings using fine-tuned model\n","    \"\"\"\n","    # Use the base model for embeddings (not the MLM head)\n","    base_model = model.bert if hasattr(model, 'bert') else model.base_model\n","\n","    inputs = tokenizer(\n","        text,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        padding=True,\n","        max_length=512,\n","        return_attention_mask=True\n","    )\n","\n","    device = next(model.parameters()).device\n","    inputs = {key: value.to(device) for key, value in inputs.items()}\n","\n","    with torch.no_grad():\n","        outputs = base_model(**inputs)\n","\n","    if method == \"cls\":\n","        embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n","    elif method == \"mean\":\n","        attention_mask = inputs['attention_mask']\n","        token_embeddings = outputs.last_hidden_state\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n","        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n","        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n","        embedding = (sum_embeddings / sum_mask).cpu().numpy()\n","    elif method == \"pooler\":\n","        embedding = outputs.pooler_output.cpu().numpy()\n","    else:\n","        raise ValueError(\"Method must be 'mean', 'cls', or 'pooler'\")\n","\n","    return embedding.squeeze()\n","\n","def main():\n","    # Set device\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    print(f\"Using device: {device}\")\n","\n","    BASE_DIR = \"/content/drive/Shareddrives/NLP/transcriptions\"\n","    TEXT_DIR = os.path.join(BASE_DIR, \"all_transcriptions\")\n","    FINE_TUNED_DIR = os.path.join(BASE_DIR, \"fine_tuned_bert_multilingual\")\n","\n","    # Load BERT Multilingual model for masked language modeling\n","    model_name = \"bert-base-multilingual-uncased\"\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForMaskedLM.from_pretrained(model_name)\n","\n","    print(\"Original BERT Multilingual model loaded successfully!\")\n","\n","    # Prepare data for fine-tuning\n","    print(\"Preparing data for fine-tuning...\")\n","    train_texts, val_texts = prepare_fine_tuning_data(TEXT_DIR)\n","\n","    print(f\"Training samples: {len(train_texts)}\")\n","    print(f\"Validation samples: {len(val_texts)}\")\n","\n","    # Fine-tune the model\n","    print(\"Starting fine-tuning...\")\n","    fine_tune_mlm(\n","        model=model,\n","        tokenizer=tokenizer,\n","        train_texts=train_texts,\n","        val_texts=val_texts,\n","        output_dir=FINE_TUNED_DIR,\n","        batch_size=8,\n","        epochs=3,\n","        learning_rate=2e-5\n","    )\n","\n","    # Load fine-tuned model for embedding generation\n","    print(\"Loading fine-tuned model...\")\n","    fine_tuned_model = AutoModelForMaskedLM.from_pretrained(FINE_TUNED_DIR)\n","    fine_tuned_model.to(device)\n","    fine_tuned_model.eval()\n","\n","    # Generate embeddings with fine-tuned model\n","    def process_with_fine_tuned(text_dir, output_file=\"fine_tuned_bert_multilingual_embeddings.csv\"):\n","        text_files = [f for f in os.listdir(text_dir) if f.endswith('.txt')]\n","        embeddings_data = []\n","\n","        for filename in tqdm(text_files, desc=\"Generating fine-tuned embeddings\"):\n","            file_path = os.path.join(text_dir, filename)\n","\n","            try:\n","                with open(file_path, 'r', encoding='utf-8') as f:\n","                    transcript = f.read().strip()\n","\n","                if not transcript:\n","                    continue\n","\n","                # Get embeddings using fine-tuned model\n","                mean_embedding = get_fine_tuned_embeddings(transcript, fine_tuned_model, tokenizer, \"mean\")\n","                cls_embedding = get_fine_tuned_embeddings(transcript, fine_tuned_model, tokenizer, \"cls\")\n","\n","                embeddings_data.append({\n","                    'filename': filename,\n","                    'text_length': len(transcript),\n","                    'mean_embedding': mean_embedding.tolist(),\n","                    'cls_embedding': cls_embedding.tolist(),\n","                    'embedding_dim': len(mean_embedding)\n","                })\n","\n","            except Exception as e:\n","                print(f\"Error processing {filename}: {str(e)}\")\n","                continue\n","\n","        df = pd.DataFrame(embeddings_data)\n","        output_path = os.path.join(BASE_DIR, output_file)\n","        df.to_csv(output_path, index=False)\n","        print(f\"Fine-tuned embeddings saved to {output_path}\")\n","        return df\n","\n","    # Generate embeddings\n","    print(\"Generating embeddings with fine-tuned model...\")\n","    fine_tuned_embeddings = process_with_fine_tuned(TEXT_DIR)\n","\n","    print(\"Fine-tuning and embedding generation completed!\")\n","\n","    # Compare embeddings before and after fine-tuning\n","    print(\"\\n=== COMPARACIÓN DE EMBEDDINGS ===\")\n","    if len(fine_tuned_embeddings) > 0:\n","        first_embedding = fine_tuned_embeddings['mean_embedding'].iloc[0]\n","        if isinstance(first_embedding, str):\n","            first_embedding_array = np.array(eval(first_embedding))\n","        else:\n","            first_embedding_array = np.array(first_embedding)\n","\n","        print(f\"Fine-tuned embedding shape: {first_embedding_array.shape}\")\n","        print(f\"Fine-tuned embedding sample (first 10 dims): {first_embedding_array[:10]}\")\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xD9eTOwlXVLp","executionInfo":{"status":"ok","timestamp":1761765346366,"user_tz":-60,"elapsed":79628,"user":{"displayName":"Leire Larrauri Olarte","userId":"15613160182125551834"}},"outputId":"a09611ef-3768-4d4e-a06f-42be073d7ed7"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Original BERT Multilingual model loaded successfully!\n","Preparing data for fine-tuning...\n","Training samples: 83\n","Validation samples: 21\n","Starting fine-tuning...\n","Epoch 1/3\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 11/11 [00:17<00:00,  1.58s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Average training loss: 2.6946\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 3/3 [00:01<00:00,  1.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Average validation loss: 0.1517\n","Epoch 2/3\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 11/11 [00:17<00:00,  1.62s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Average training loss: 2.5787\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 3/3 [00:01<00:00,  1.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Average validation loss: 0.0967\n","Epoch 3/3\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 11/11 [00:17<00:00,  1.63s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Average training loss: 2.5385\n"]},{"output_type":"stream","name":"stderr","text":["Validation: 100%|██████████| 3/3 [00:01<00:00,  1.78it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Average validation loss: 0.0619\n","Fine-tuned model saved to /content/drive/Shareddrives/NLP/transcriptions/fine_tuned_bert_multilingual\n","Loading fine-tuned model...\n","Generating embeddings with fine-tuned model...\n"]},{"output_type":"stream","name":"stderr","text":["Generating fine-tuned embeddings: 100%|██████████| 104/104 [00:15<00:00,  6.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Fine-tuned embeddings saved to /content/drive/Shareddrives/NLP/transcriptions/fine_tuned_bert_multilingual_embeddings.csv\n","Fine-tuning and embedding generation completed!\n","\n","=== COMPARACIÓN DE EMBEDDINGS ===\n","Fine-tuned embedding shape: (768,)\n","Fine-tuned embedding sample (first 10 dims): [-0.46022147 -0.34349376  0.20957237  0.23939344  0.29230952 -0.11387414\n"," -0.00508887 -0.37776092 -0.06174021  0.19991039]\n"]}]},{"cell_type":"markdown","source":["pre-trained vs fine-tuned"],"metadata":{"id":"PmMk1-6qaeuu"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","def compare_embeddings(original_csv, fine_tuned_csv):\n","    \"\"\"Compare original and fine-tuned embeddings\"\"\"\n","    orig_df = pd.read_csv(original_csv)\n","    fine_df = pd.read_csv(fine_tuned_csv)\n","\n","    # Ensure we're comparing the same files\n","    common_files = set(orig_df['filename']).intersection(set(fine_df['filename']))\n","    orig_df = orig_df[orig_df['filename'].isin(common_files)].sort_values('filename')\n","    fine_df = fine_df[fine_df['filename'].isin(common_files)].sort_values('filename')\n","\n","    print(f\"Comparing {len(common_files)} common files\")\n","\n","    # Convert embeddings to arrays\n","    def safe_convert(embedding_str):\n","        if isinstance(embedding_str, str):\n","            return np.array(eval(embedding_str))\n","        return np.array(embedding_str)\n","\n","    orig_embeddings = [safe_convert(x) for x in orig_df['mean_embedding']]\n","    fine_embeddings = [safe_convert(x) for x in fine_df['mean_embedding']]\n","\n","    # Calculate similarities\n","    similarities = []\n","    for orig, fine in zip(orig_embeddings, fine_embeddings):\n","        sim = cosine_similarity([orig], [fine])[0][0]\n","        similarities.append(sim)\n","\n","    print(f\"Average cosine similarity between original and fine-tuned: {np.mean(similarities):.4f}\")\n","    print(f\"Similarity std: {np.std(similarities):.4f}\")\n","    print(f\"Min similarity: {np.min(similarities):.4f}\")\n","    print(f\"Max similarity: {np.max(similarities):.4f}\")\n","\n","    return similarities, orig_embeddings, fine_embeddings, orig_df, fine_df\n","def comparison(original_csv, fine_tuned_csv):\n","    \"\"\" comparison of embeddings\"\"\"\n","    similarities, _, _, _, _ = compare_embeddings(original_csv, fine_tuned_csv)\n","\n","    # Basic statistics\n","    print(\"\\n=== SIMILARITY ANALYSIS ===\")\n","    print(f\"Mean similarity: {np.mean(similarities):.4f}\")\n","    print(f\"Median similarity: {np.median(similarities):.4f}\")\n","    print(f\"Similarity range: [{np.min(similarities):.4f}, {np.max(similarities):.4f}]\")\n","\n","    # Interpretation\n","    mean_sim = np.mean(similarities)\n","    if mean_sim > 0.95:\n","        print(\"Interpretation: Very similar embeddings - fine-tuning had minimal effect\")\n","    elif mean_sim > 0.8:\n","        print(\"Interpretation: Similar embeddings - moderate fine-tuning effect\")\n","    elif mean_sim > 0.6:\n","        print(\"Interpretation: Different embeddings - significant fine-tuning effect\")\n","    else:\n","        print(\"Interpretation: Very different embeddings - major fine-tuning effect\")\n","\n","    return similarities\n","\n","# Uso\n","BASE_DIR = \"/content/drive/Shareddrives/NLP/transcriptions\"\n","similarities = comparison(\n","    f\"{BASE_DIR}/bert_multilingual_embeddings.csv\",\n","    f\"{BASE_DIR}/fine_tuned_bert_multilingual_embeddings.csv\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iDgVDY7jaeH_","executionInfo":{"status":"ok","timestamp":1761765775564,"user_tz":-60,"elapsed":610,"user":{"displayName":"Leire Larrauri Olarte","userId":"15613160182125551834"}},"outputId":"7a5a81cf-5ad9-47ab-ae05-c9f128047ac3"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Comparing 104 common files\n","Average cosine similarity between original and fine-tuned: 0.9654\n","Similarity std: 0.0050\n","Min similarity: 0.9499\n","Max similarity: 0.9744\n","\n","=== SIMILARITY ANALYSIS ===\n","Mean similarity: 0.9654\n","Median similarity: 0.9652\n","Similarity range: [0.9499, 0.9744]\n","Interpretation: Very similar embeddings - fine-tuning had minimal effect\n"]}]}]}